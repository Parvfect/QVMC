{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99d2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094b1565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-0.5851, dtype=torch.float64), tensor(2.5131, dtype=torch.float64), tensor(0.6771, dtype=torch.float64), tensor(-2.5998, dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define alphas as torch tensors with requires_grad=True\n",
    "alpha_1 = torch.tensor(1.013, dtype=torch.float64, requires_grad=True)\n",
    "alpha_2 = torch.tensor(0.2119, dtype=torch.float64, requires_grad=True)\n",
    "alpha_3 = torch.tensor(0.1406, dtype=torch.float64, requires_grad=True)\n",
    "alpha_4 = torch.tensor(0.003, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "alphas = [alpha_1, alpha_2, alpha_3, alpha_4]\n",
    "\n",
    "# psi function using torch\n",
    "def psi(x, y, alpha_1, alpha_2, alpha_3, alpha_4):\n",
    "    r1 = torch.norm(x)\n",
    "    r2 = torch.norm(y)\n",
    "    r12 = torch.norm(x - y)\n",
    "\n",
    "    term1 = torch.exp(-2 * (r1 + r2))\n",
    "    term2 = 1 + 0.5 * r12 * torch.exp(-alpha_1 * r12)\n",
    "    term3 = 1 + alpha_2 * (r1 + r2) * r12 + alpha_3 * (r1 - r2)**2 - alpha_4 * r12\n",
    "\n",
    "    return term1 * term2 * term3\n",
    "\n",
    "# Tensors with gradients enabled\n",
    "x = torch.tensor([0.2, 0.3, 0.1], dtype=torch.float64, requires_grad=True)\n",
    "y = torch.tensor([0.1, 0.4, 0.3], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# Compute psi\n",
    "psi_val = psi(x, y, *alphas)\n",
    "\n",
    "# First gradients w.r.t. x and y\n",
    "grad_x = torch.autograd.grad(psi_val, x, create_graph=True)[0]\n",
    "grad_y = torch.autograd.grad(psi_val, y, create_graph=True)[0]\n",
    "\n",
    "# Laplacians\n",
    "laplacian_x = sum(torch.autograd.grad(grad_x[i], x, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "laplacian_y = sum(torch.autograd.grad(grad_y[i], y, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "\n",
    "lap = laplacian_x + laplacian_y\n",
    "# Derivatives of Laplacians w.r.t. alphas\n",
    "dlap_dalpha = torch.autograd.grad(lap, alphas, retain_graph=True)\n",
    "\n",
    "\n",
    "print(dlap_dalpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bb34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def local_energy(psi_val, r1, r2):\n",
    "    grad_x = torch.autograd.grad(psi_val, r1, create_graph=True)[0]\n",
    "    grad_y = torch.autograd.grad(psi_val, r2, create_graph=True)[0]\n",
    "\n",
    "    # Laplacians\n",
    "    laplacian_x = sum(torch.autograd.grad(grad_x[i], r1, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "    laplacian_y = sum(torch.autograd.grad(grad_y[i], r2, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "\n",
    "    ke = - 0.5 * (laplacian_x + laplacian_y)\n",
    "\n",
    "    return - 2 / torch.norm(r1) - 2 / torch.norm(r2) + 1 / torch.norm(r1 - r2) + ke\n",
    "\"\"\"\n",
    "\n",
    "def local_energy(psi_val, r1, r2):\n",
    "    grad_r1 = torch.autograd.grad(psi_val, r1, create_graph=True)[0]\n",
    "    grad_r2 = torch.autograd.grad(psi_val, r2, create_graph=True)[0]\n",
    "\n",
    "    lap_r1 = torch.autograd.grad(grad_r1, r1, grad_outputs=torch.ones_like(grad_r1), create_graph=True)[0]\n",
    "    lap_r2 = torch.autograd.grad(grad_r2, r2, grad_outputs=torch.ones_like(grad_r2), create_graph=True)[0]\n",
    "\n",
    "    laplacian = lap_r1.sum() + lap_r2.sum()\n",
    "    ke = -0.5 * laplacian\n",
    "\n",
    "    potential = -2 / torch.norm(r1) - 2 / torch.norm(r2) + 1 / torch.norm(r1 - r2)\n",
    "\n",
    "    return ke + potential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea8fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2314.54it/s]\n"
     ]
    }
   ],
   "source": [
    "L = 1\n",
    "r1 = torch.rand(3, requires_grad=True) * 2 * L - L\n",
    "r2 = torch.rand(3, requires_grad=True) * 2 * L - L #random number from -L to L\n",
    "E = 0\n",
    "E2 = 0\n",
    "Eln_average = 0\n",
    "ln_average = 0\n",
    "rejection_ratio = 0\n",
    "step = 0\n",
    "max_steps = 500\n",
    "N = 10000\n",
    "dlap_dalpha = 0\n",
    "xs = []\n",
    "ys = []\n",
    "psi_vals = []\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    flag = False\n",
    "    chose = np.random.rand()\n",
    "    step = step + 1\n",
    "    if chose < 0.5:\n",
    "        r1_trial = r1 + 0.5 * (torch.rand(3) * 2 * L-L)\n",
    "        r2_trial = r2\n",
    "    else:\n",
    "        r2_trial = r2 + 0.5 * (torch.rand(3) * 2 * L-L)\n",
    "        r1_trial = r1\n",
    "\n",
    "\n",
    "    psi_val = psi(r1, r2, *alphas)\n",
    "    psi_val_trial = psi(r1_trial, r2_trial, *alphas)\n",
    "    \n",
    "\n",
    "    if psi_val_trial.item() >= psi_val.item():\n",
    "        r1 = r1_trial\n",
    "        r2 = r2_trial\n",
    "        flag = True\n",
    "    \n",
    "    else:\n",
    "        dummy = np.random.rand()\n",
    "        if dummy < psi_val_trial.item() / psi_val.item():\n",
    "            r1 = r1_trial\n",
    "            r2 = r2_trial\n",
    "        else:\n",
    "            rejection_ratio += 1./N\n",
    "            \n",
    "    if step > max_steps:\n",
    "        xs.append(r1)\n",
    "        ys.append(r2)\n",
    "\n",
    "        if flag:\n",
    "            psi_vals.append(psi_val_trial)\n",
    "        else:\n",
    "            psi_vals.append(psi_val)\n",
    "        \"\"\"\n",
    "        local_E = local_energy(psi_val_trial, r1, r2)\n",
    "        E += local_E / (N - max_steps)\n",
    "        E2 += local_E ** 2 / (N - max_steps)\n",
    "        dlap_dalpha += torch.tensor(torch.autograd.grad(local_E, alphas, retain_graph=True)) / (N - max_steps)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac183ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_psi(x, y, alpha_1, alpha_2, alpha_3, alpha_4):\n",
    "    r1 = torch.norm(x, dim=1)\n",
    "    r2 = torch.norm(y, dim=1)\n",
    "    r12 = torch.norm(x - y, dim=1)\n",
    "\n",
    "    term1 = torch.exp(-2 * (r1 + r2))\n",
    "    term2 = 1 + 0.5 * r12 * torch.exp(-alpha_1 * r12)\n",
    "    term3 = 1 + alpha_2 * (r1 + r2) * r12 + alpha_3 * (r1 - r2)**2 - alpha_4 * r12\n",
    "\n",
    "    return term1 * term2 * term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b94a09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_1 = torch.tensor(1.013, dtype=torch.float64, requires_grad=True)\n",
    "alpha_2 = torch.tensor(0.2119, dtype=torch.float64, requires_grad=True)\n",
    "alpha_3 = torch.tensor(0.1406, dtype=torch.float64, requires_grad=True)\n",
    "alpha_4 = torch.tensor(0.003, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "def batched_psi(x, y):\n",
    "    r1 = torch.norm(x, dim=1)\n",
    "    r2 = torch.norm(y, dim=1)\n",
    "    r12 = torch.norm(x - y, dim=1)\n",
    "\n",
    "    term1 = torch.exp(-2 * (r1 + r2))\n",
    "    term2 = 1 + 0.5 * r12 * torch.exp(-alpha_1 * r12)\n",
    "    term3 = 1 + alpha_2 * (r1 + r2) * r12 + alpha_3 * (r1 - r2)**2 - alpha_4 * r12\n",
    "\n",
    "    return term1 * term2 * term3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbcb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c01a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = batched_psi(torch.stack(xs), torch.stack(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "185c6727",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:385\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    378\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[1;32m--> 385\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(t, torch.stack(xs[:5]), is_grads_batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d30199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import vmap, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2552436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_grad_f = vmap(grad(batched_psi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a804090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = torch.autograd.functional.jacobian(batched_psi, (torch.stack(xs), torch.stack(ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ad9ed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9500, 9500, 3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "459d67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import grad, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1ab1654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "# Original scalar-valued psi\n",
    "def psi(X):\n",
    "    x = X[:3]\n",
    "    y = X[3:6]\n",
    "    alpha_1, alpha_2, alpha_3, alpha_4 = X[6:]\n",
    "    r1 = torch.norm(x)\n",
    "    r2 = torch.norm(y)\n",
    "    r12 = torch.norm(x - y)\n",
    "\n",
    "    term1 = torch.exp(-2 * (r1 + r2))\n",
    "    term2 = 1 + 0.5 * r12 * torch.exp(-alpha_1 * r12)\n",
    "    term3 = 1 + alpha_2 * (r1 + r2) * r12 + alpha_3 * (r1 - r2)**2 - alpha_4 * r12\n",
    "\n",
    "    return term1 * term2 * term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "396492fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def third_partial_derivatives(scalar_func, inp):\n",
    "    return jacobian(\n",
    "        lambda x: jacobian(\n",
    "            lambda y: jacobian(scalar_func, y), \n",
    "            x\n",
    "        ), \n",
    "        inp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "66f83d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = vmap(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6cadd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(len(xs)):\n",
    "    inputs.append(torch.tensor([*xs[i], *ys[i], alpha_1, alpha_2, alpha_3, alpha_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2cfb4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.stack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e7aaf142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7768,  0.0990,  0.2629, -1.0013,  2.1483,  0.9251,  1.0130,  0.2119,\n",
       "         0.1406,  0.0030], dtype=torch.float64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a009f0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0039, 0.0046, 0.0035,  ..., 0.0008, 0.0013, 0.0020],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "321df7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = vmap(grad(psi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "443e40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs(x):\n",
    "    return torch.exp(x[0] + x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4f31236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = vmap(grad(grad(fs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ac32030",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = vmap(grad(grad(fs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a8e3699f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.0855)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(grad(fs))(torch.tensor(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = vmap(hessian(psi))\n",
    "E = torch.diagonal(hess_full[0]).sum()\n",
    "grad(E, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1c986531",
   "metadata": {},
   "outputs": [],
   "source": [
    "hess_full = t(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb392cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hess_full[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d240498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = torch.diagonal(hess_full[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5f6a9692",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "jacobian() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lap_alpha \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: jacobian() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "lap_alpha = jacobian(hessian(psi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56ba4bf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 2 dims. Maybe you wanted to use the vjp or jacrev APIs instead?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlap_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\_functorch\\apis.py:363\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meager_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\_functorch\\eager_transforms.py:1285\u001b[0m, in \u001b[0;36mgrad_impl\u001b[1;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[1;32m-> 1285\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_and_value_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[0;32m   1287\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\_functorch\\vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\_functorch\\eager_transforms.py:1262\u001b[0m, in \u001b[0;36mgrad_and_value_impl\u001b[1;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1260\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto return a Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1263\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto return a scalar Tensor, got tensor with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1264\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims. Maybe you wanted to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1265\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse the vjp or jacrev APIs instead?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1267\u001b[0m flat_diff_args, spec \u001b[38;5;241m=\u001b[39m tree_flatten(diff_args)\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;66;03m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 2 dims. Maybe you wanted to use the vjp or jacrev APIs instead?"
     ]
    }
   ],
   "source": [
    "lap_alpha(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75e22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
