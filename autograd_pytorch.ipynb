{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e99d2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b1565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(-0.5851, dtype=torch.float64), tensor(2.5131, dtype=torch.float64), tensor(0.6771, dtype=torch.float64), tensor(-2.5998, dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define alphas as torch tensors with requires_grad=True\n",
    "alpha_1 = torch.tensor(1.013, dtype=torch.float64, requires_grad=True)\n",
    "alpha_2 = torch.tensor(0.2119, dtype=torch.float64, requires_grad=True)\n",
    "alpha_3 = torch.tensor(0.1406, dtype=torch.float64, requires_grad=True)\n",
    "alpha_4 = torch.tensor(0.003, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "alphas = [alpha_1, alpha_2, alpha_3, alpha_4]\n",
    "\n",
    "# psi function using torch\n",
    "def psi(x, y, alpha_1, alpha_2, alpha_3, alpha_4):\n",
    "    r1 = torch.norm(x)\n",
    "    r2 = torch.norm(y)\n",
    "    r12 = torch.norm(x - y)\n",
    "\n",
    "    term1 = torch.exp(-2 * (r1 + r2))\n",
    "    term2 = 1 + 0.5 * r12 * torch.exp(-alpha_1 * r12)\n",
    "    term3 = 1 + alpha_2 * (r1 + r2) * r12 + alpha_3 * (r1 - r2)**2 - alpha_4 * r12\n",
    "\n",
    "    return term1 * term2 * term3\n",
    "\n",
    "# Tensors with gradients enabled\n",
    "x = torch.tensor([0.2, 0.3, 0.1], dtype=torch.float64, requires_grad=True)\n",
    "y = torch.tensor([0.1, 0.4, 0.3], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# Compute psi\n",
    "psi_val = psi(x, y, *alphas)\n",
    "\n",
    "# First gradients w.r.t. x and y\n",
    "grad_x = torch.autograd.grad(psi_val, x, create_graph=True)[0]\n",
    "grad_y = torch.autograd.grad(psi_val, y, create_graph=True)[0]\n",
    "\n",
    "# Laplacians\n",
    "laplacian_x = sum(torch.autograd.grad(grad_x[i], x, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "laplacian_y = sum(torch.autograd.grad(grad_y[i], y, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "\n",
    "lap = laplacian_x + laplacian_y\n",
    "# Derivatives of Laplacians w.r.t. alphas\n",
    "dlap_dalpha = torch.autograd.grad(lap, alphas, retain_graph=True)\n",
    "\n",
    "\n",
    "print(dlap_dalpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "28bb34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def local_energy(psi_val, r1, r2):\n",
    "    grad_x = torch.autograd.grad(psi_val, r1, create_graph=True)[0]\n",
    "    grad_y = torch.autograd.grad(psi_val, r2, create_graph=True)[0]\n",
    "\n",
    "    # Laplacians\n",
    "    laplacian_x = sum(torch.autograd.grad(grad_x[i], r1, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "    laplacian_y = sum(torch.autograd.grad(grad_y[i], r2, retain_graph=True, create_graph=True)[0][i] for i in range(3))\n",
    "\n",
    "    ke = - 0.5 * (laplacian_x + laplacian_y)\n",
    "\n",
    "    return - 2 / torch.norm(r1) - 2 / torch.norm(r2) + 1 / torch.norm(r1 - r2) + ke\n",
    "\"\"\"\n",
    "\n",
    "def local_energy(psi_val, r1, r2):\n",
    "    grad_r1 = torch.autograd.grad(psi_val, r1, create_graph=True)[0]\n",
    "    grad_r2 = torch.autograd.grad(psi_val, r2, create_graph=True)[0]\n",
    "\n",
    "    lap_r1 = torch.autograd.grad(grad_r1, r1, grad_outputs=torch.ones_like(grad_r1), create_graph=True)[0]\n",
    "    lap_r2 = torch.autograd.grad(grad_r2, r2, grad_outputs=torch.ones_like(grad_r2), create_graph=True)[0]\n",
    "\n",
    "    laplacian = lap_r1.sum() + lap_r2.sum()\n",
    "    ke = -0.5 * laplacian\n",
    "\n",
    "    potential = -2 / torch.norm(r1) - 2 / torch.norm(r2) + 1 / torch.norm(r1 - r2)\n",
    "\n",
    "    return ke + potential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1ea8fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2299.97it/s]\n"
     ]
    }
   ],
   "source": [
    "L = 1\n",
    "r1 = torch.rand(3, requires_grad=True) * 2 * L - L\n",
    "r2 = torch.rand(3, requires_grad=True) * 2 * L - L #random number from -L to L\n",
    "E = 0\n",
    "E2 = 0\n",
    "Eln_average = 0\n",
    "ln_average = 0\n",
    "rejection_ratio = 0\n",
    "step = 0\n",
    "max_steps = 500\n",
    "N = 10000\n",
    "dlap_dalpha = 0\n",
    "xs = []\n",
    "ys = []\n",
    "psi_vals = []\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    flag = False\n",
    "    chose = np.random.rand()\n",
    "    step = step + 1\n",
    "    if chose < 0.5:\n",
    "        r1_trial = r1 + 0.5 * (torch.rand(3) * 2 * L-L)\n",
    "        r2_trial = r2\n",
    "    else:\n",
    "        r2_trial = r2 + 0.5 * (torch.rand(3) * 2 * L-L)\n",
    "        r1_trial = r1\n",
    "\n",
    "\n",
    "    psi_val = psi(r1, r2, *alphas)\n",
    "    psi_val_trial = psi(r1_trial, r2_trial, *alphas)\n",
    "    \n",
    "\n",
    "    if psi_val_trial.item() >= psi_val.item():\n",
    "        r1 = r1_trial\n",
    "        r2 = r2_trial\n",
    "        flag = True\n",
    "    \n",
    "    else:\n",
    "        dummy = np.random.rand()\n",
    "        if dummy < psi_val_trial.item() / psi_val.item():\n",
    "            r1 = r1_trial\n",
    "            r2 = r2_trial\n",
    "        else:\n",
    "            rejection_ratio += 1./N\n",
    "            \n",
    "    if step > max_steps:\n",
    "        xs.append(r1)\n",
    "        ys.append(r2)\n",
    "\n",
    "        if flag:\n",
    "            psi_vals.append(psi_val_trial)\n",
    "        else:\n",
    "            psi_vals.append(psi_val)\n",
    "        \"\"\"\n",
    "        local_E = local_energy(psi_val_trial, r1, r2)\n",
    "        E += local_E / (N - max_steps)\n",
    "        E2 += local_E ** 2 / (N - max_steps)\n",
    "        dlap_dalpha += torch.tensor(torch.autograd.grad(local_E, alphas, retain_graph=True)) / (N - max_steps)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cac183ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_psi(x, y, alpha_1, alpha_2, alpha_3, alpha_4):\n",
    "    r1 = torch.norm(x, dim=1)\n",
    "    r2 = torch.norm(y, dim=1)\n",
    "    r12 = torch.norm(x - y, dim=1)\n",
    "\n",
    "    term1 = torch.exp(-2 * (r1 + r2))\n",
    "    term2 = 1 + 0.5 * r12 * torch.exp(-alpha_1 * r12)\n",
    "    term3 = 1 + alpha_2 * (r1 + r2) * r12 + alpha_3 * (r1 - r2)**2 - alpha_4 * r12\n",
    "\n",
    "    return term1 * term2 * term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b94a09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_local_energy(r1_batch, r2_batch, *alphas):\n",
    "    r1_batch = r1_batch.detach().requires_grad_()\n",
    "    r2_batch = r2_batch.detach().requires_grad_()\n",
    "    psi_batch = psi(r1_batch, r2_batch, *alphas)\n",
    "\n",
    "    grad_r1 = torch.autograd.grad(psi_batch, r1_batch, grad_outputs=torch.ones_like(psi_batch), create_graph=True)[0]\n",
    "    grad_r2 = torch.autograd.grad(psi_batch, r2_batch, grad_outputs=torch.ones_like(psi_batch), create_graph=True)[0]\n",
    "\n",
    "    lap_r1 = torch.zeros_like(psi_batch)\n",
    "    lap_r2 = torch.zeros_like(psi_batch)\n",
    "\n",
    "    for i in range(3):\n",
    "        lap_r1 += torch.autograd.grad(grad_r1[:, i], r1_batch, grad_outputs=torch.ones_like(psi_batch), retain_graph=True, create_graph=True)[0][:, i]\n",
    "        lap_r2 += torch.autograd.grad(grad_r2[:, i], r2_batch, grad_outputs=torch.ones_like(psi_batch), retain_graph=True, create_graph=True)[0][:, i]\n",
    "\n",
    "    kinetic = -0.5 * (lap_r1 + lap_r2)\n",
    "    potential = (\n",
    "        -2 / torch.norm(r1_batch, dim=1)\n",
    "        -2 / torch.norm(r2_batch, dim=1)\n",
    "        +1 / torch.norm(r1_batch - r2_batch, dim=1)\n",
    "    )\n",
    "\n",
    "    return kinetic + potential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d7d332bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([]) and output[0] has a shape of torch.Size([5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[193], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m local_E \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_local_energy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43malphas\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[191], line 13\u001b[0m, in \u001b[0;36mbatched_local_energy\u001b[1;34m(r1_batch, r2_batch, *alphas)\u001b[0m\n\u001b[0;32m     10\u001b[0m lap_r2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(psi_batch)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     lap_r1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_r1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr1_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsi_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, i]\n\u001b[0;32m     14\u001b[0m     lap_r2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(grad_r2[:, i], r2_batch, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(psi_batch), retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:, i]\n\u001b[0;32m     16\u001b[0m kinetic \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (lap_r1 \u001b[38;5;241m+\u001b[39m lap_r2)\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:385\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    378\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[1;32m--> 385\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:104\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         )\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in shape: grad_output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grads\u001b[38;5;241m.\u001b[39mindex(grad))\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grad_shape)\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mindex(out))\n\u001b[0;32m    111\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(out_shape)\n\u001b[0;32m    113\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m         )\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor complex Tensors, both grad_output and output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are required to have the same dtype.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([]) and output[0] has a shape of torch.Size([5])."
     ]
    }
   ],
   "source": [
    "local_E = batched_local_energy(torch.stack(xs[:5]), torch.stack(ys[:5]), *alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e6cccb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.7991, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4019a0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8893, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c45ecc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0043,  0.0316,  0.0081, -0.0188], dtype=torch.float64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlap_dalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91719b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
