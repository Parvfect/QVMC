{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a432f0",
   "metadata": {},
   "source": [
    "## JAX VMC\n",
    "Code taken from - https://teddykoker.com/2024/11/neural-vmc-jax/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c27d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fae99",
   "metadata": {},
   "source": [
    "### Local energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9ce0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_energy(wavefunction, atoms, charges, pos):\n",
    "    return kinetic_energy(wavefunction, pos) + potential_energy(atoms, charges, pos)\n",
    "\n",
    "def kinetic_energy(wavefunction, pos):\n",
    "    \"\"\"Kinetic energy term of Hamiltonian\"\"\"\n",
    "    laplacian = jnp.trace(jax.hessian(wavefunction)(pos))\n",
    "    return -0.5 * laplacian / wavefunction(pos)\n",
    "\n",
    "def potential_energy(pos):\n",
    "\n",
    "    r1 = jnp.linalg.norm(pos[:, :, :3], axis=-1)\n",
    "    r2 = jnp.linalg.norm(pos[:, :, 3:], axis=-1)\n",
    "    r12 = jnp.linalg.norm(pos[:, :, :3] - pos[:, :, 3:], axis=-1)\n",
    "    \n",
    "    return -2 / (r1) - 2 / (r2) + 1 / (r12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecedc2",
   "metadata": {},
   "source": [
    "### Metropolis sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf084988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "from functools import partial\n",
    "from collections.abc import Callable\n",
    "\n",
    "@eqx.filter_jit\n",
    "@partial(jax.vmap, in_axes=(None, 0, None, None, 0))\n",
    "def metropolis(\n",
    "    wavefunction: Callable,\n",
    "    pos: jax.Array,\n",
    "    step_size: float,\n",
    "    mcmc_steps: int,\n",
    "    key: jax.Array,\n",
    "):\n",
    "    \"\"\"MCMC step\n",
    "\n",
    "    Args:\n",
    "        wavefunction: neural wavefunction\n",
    "        pos: [3N] current electron positions flattened\n",
    "        step_size: std of proposal for metropolis sampling\n",
    "        mcmc_steps: number of steps to perform\n",
    "        key: random key\n",
    "    \"\"\"\n",
    "\n",
    "    def step(_, carry):\n",
    "        pos, prob, num_accepts, key = carry\n",
    "        key, subkey = jax.random.split(key)\n",
    "        pos_proposal = pos + step_size * jax.random.normal(subkey, shape=pos.shape)\n",
    "        prob_proposal = wavefunction(pos_proposal) ** 2\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        accept = jax.random.uniform(subkey) < prob_proposal / prob\n",
    "        prob = jnp.where(accept, prob_proposal, prob)\n",
    "        pos = jnp.where(accept, pos_proposal, pos)\n",
    "        num_accepts = num_accepts + jnp.sum(accept)\n",
    "\n",
    "        return pos, prob, num_accepts, key\n",
    "\n",
    "    prob = wavefunction(pos) ** 2\n",
    "    carry = (pos, prob, 0, key)\n",
    "    pos, prob, num_accepts, key = jax.lax.fori_loop(0, mcmc_steps, step, carry)\n",
    "    return pos, num_accepts / mcmc_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015f0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavefunction_h(pos):\n",
    "    return jnp.exp(-jnp.linalg.norm(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900081f",
   "metadata": {},
   "source": [
    "### Neural network, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2af69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss(atoms, charges):\n",
    "    # Based on implementation in https://github.com/google-deepmind/ferminet/\n",
    "\n",
    "    @eqx.filter_custom_jvp\n",
    "    def total_energy(wavefunction, pos):\n",
    "        \"\"\"Define L()\"\"\"\n",
    "        batch_local_energy = jax.vmap(local_energy, (None, None, None, 0))\n",
    "        e_l = batch_local_energy(wavefunction, atoms, charges, pos)\n",
    "        loss = jnp.mean(e_l)\n",
    "        return loss, e_l\n",
    "\n",
    "    @total_energy.def_jvp\n",
    "    def total_energy_jvp(primals, tangents):\n",
    "        \"\"\"Define the gradient of L()\"\"\"\n",
    "        wavefunction, pos = primals\n",
    "        log_wavefunction = lambda psi, pos: jnp.log(psi(pos))\n",
    "        batch_wavefunction = jax.vmap(log_wavefunction, (None, 0))\n",
    "        psi_primal, psi_tangent = eqx.filter_jvp(batch_wavefunction, primals, tangents)\n",
    "        loss, local_energy = total_energy(wavefunction, pos)\n",
    "        primals_out = loss, local_energy\n",
    "        batch_size = jnp.shape(local_energy)[0]\n",
    "        tangents_out = (jnp.dot(psi_tangent, local_energy - loss) / batch_size, local_energy)\n",
    "        return primals_out, tangents_out\n",
    "\n",
    "    return total_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c770a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(eqx.Module):\n",
    "    \"\"\"Linear layer\"\"\"\n",
    "\n",
    "    weights: jax.Array\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, key):\n",
    "        lim = math.sqrt(1 / (in_size + out_size))\n",
    "        self.weights = jax.random.uniform(key, (in_size, out_size), minval=-lim, maxval=lim)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.weights) + self.bias\n",
    "\n",
    "\n",
    "class PsiMLP(eqx.Module):\n",
    "    \"\"\"Simple MLP-based model using Slater determinant\"\"\"\n",
    "\n",
    "    spins: tuple[int, int]\n",
    "    linears: list[Linear]\n",
    "    orbitals: Linear\n",
    "    sigma: jax.Array \n",
    "    pi: jax.Array\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_sizes: list[int],\n",
    "        spins: tuple[int, int],\n",
    "        determinants: int,\n",
    "        key: jax.Array,\n",
    "    ):\n",
    "        num_atoms = 1  # assume one atom\n",
    "        sizes = [5] + hidden_sizes  # 5 input features\n",
    "        key, *keys = jax.random.split(key, len(sizes))\n",
    "        self.linears = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(Linear(sizes[i], sizes[i + 1], keys[i]))\n",
    "        self.orbitals = Linear(sizes[-1], sum(spins) * determinants, key)\n",
    "        self.sigma = jnp.ones((num_atoms, sum(spins) * determinants))\n",
    "        self.pi = jnp.ones((num_atoms, sum(spins) * determinants))\n",
    "        self.spins = spins\n",
    "\n",
    "    def __call__(self, pos):\n",
    "        # atom electron displacement [electron, atom, 3]\n",
    "        ae = pos.reshape(-1, 1, 3)\n",
    "        # atom electron distance [electron, atom, 1]\n",
    "        r_ae = jnp.linalg.norm(ae, axis=2, keepdims=True)\n",
    "        # feature for spins; 1 for up, -1 for down [atom, 1]\n",
    "        spins = jnp.concatenate([jnp.ones(self.spins[0]), jnp.ones(self.spins[1]) * -1])\n",
    "\n",
    "        # combine into features\n",
    "        h = jnp.concatenate([ae, r_ae], axis=2)\n",
    "        h = h.reshape([h.shape[0], -1])\n",
    "        h = jnp.concatenate([h, spins[:, None]], axis=1)\n",
    "\n",
    "        # multi-layer perceptron with tanh activations\n",
    "        for linear in self.linears:\n",
    "            h = jnp.tanh(linear(h))\n",
    "\n",
    "        phi = self.orbitals(h) * jnp.sum(self.pi * jnp.exp(-self.sigma * r_ae), axis=1)\n",
    "\n",
    "        # [electron, electron * determinants] -> [determinants, electron, electron]\n",
    "        phi = phi.reshape(phi.shape[0], -1, phi.shape[0]).transpose(1, 0, 2)\n",
    "        det = jnp.linalg.det(phi)\n",
    "        return jnp.sum(det)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55ad2f",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16eb3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm import tqdm\n",
    "\n",
    "def vmc(\n",
    "    wavefunction: Callable,\n",
    "    atoms: jax.Array,\n",
    "    charges: jax.Array,\n",
    "    spins: tuple[int, int],\n",
    "    *,\n",
    "    batch_size: int = 4096,\n",
    "    mcmc_steps: int = 50,\n",
    "    warmup_steps: int = 200,\n",
    "    init_width: float = 0.4,\n",
    "    step_size: float = 0.2,\n",
    "    learning_rate: float = 3e-3,\n",
    "    iterations: int = 2_000,\n",
    "    key: jax.Array,\n",
    "):\n",
    "    \"\"\"Perform variational Monte Carlo\n",
    "\n",
    "    Args:\n",
    "        wavefunction: neural wavefunction\n",
    "        atoms: [M, 3] atomic positions\n",
    "        charges: [M] atomic charges\n",
    "        spins: number spin-up, spin-down electrons\n",
    "        batch_size: number of electron configurations to sample\n",
    "        mcmc_steps: number of mcmc steps to perform between neural network\n",
    "            updates (lessens autocorrelation)\n",
    "        warmup_steps: number of mcmc steps to perform before starting training\n",
    "        step_size: std of proposal for metropolis sampling\n",
    "        learning_rate: learning rate\n",
    "        iterations: number of neural network updates\n",
    "        key: random key\n",
    "    \"\"\"\n",
    "    total_energy = make_loss(atoms, charges)\n",
    "\n",
    "    # initialize electron positions and perform warmup mcmc steps\n",
    "    key, subkey = jax.random.split(key)\n",
    "    pos = init_width * jax.random.normal(subkey, shape=(batch_size, sum(spins) * 3))\n",
    "    key, *subkeys = jax.random.split(key, batch_size + 1)\n",
    "    pos, _ = metropolis(wavefunction, pos, step_size, warmup_steps, jnp.array(subkeys))\n",
    "\n",
    "    # Adam optimizer with gradient clipping\n",
    "    optimizer = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(learning_rate))\n",
    "    opt_state = optimizer.init(eqx.filter(wavefunction, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def train_step(wavefunction, pos, key, opt_state):\n",
    "        key, *subkeys = jax.random.split(key, batch_size + 1)\n",
    "        pos, accept = metropolis(wavefunction, pos, step_size, mcmc_steps, jnp.array(subkeys))\n",
    "        (loss, _), grads = eqx.filter_value_and_grad(total_energy, has_aux=True)(wavefunction, pos)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, wavefunction)\n",
    "        wavefunction = eqx.apply_updates(wavefunction, updates)\n",
    "        return wavefunction, pos, key, opt_state, loss, accept\n",
    "\n",
    "    losses, pmoves = [], []\n",
    "    pbar = tqdm(range(iterations))\n",
    "    for _ in pbar:\n",
    "        wavefunction, pos, key, opt_state, loss, pmove = train_step(wavefunction, pos, key, opt_state)\n",
    "        pmove = pmove.mean()\n",
    "        losses.append(loss)\n",
    "        pmoves.append(pmove)\n",
    "        pbar.set_description(f\"Energy: {loss:.4f}, P(move): {pmove:.2f}\")\n",
    "\n",
    "    return losses, pmoves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304e6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lithium at origin\n",
    "atoms = jnp.zeros((1, 2))\n",
    "charges = jnp.array([3.0])\n",
    "spins = (2, 1) # 2 spin-up, 1 spin-down electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ad109",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "#model = PsiMLP(hidden_sizes=[64, 64, 64], determinants=4, spins=spins, key=key)\n",
    "losses, _ = vmc(wavefunction_h, atoms, charges, spins, key=subkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41bc00-7829-4040-9620-ea91affe6939",
   "metadata": {},
   "source": [
    "### Ferminet tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5d673d-77b3-47a6-b7af-bce353886038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ferminet.utils import system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7ec30a5-fa27-43b3-891a-a39da74c6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ferminet import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b55731d1-c5cb-4ea4-a080-f9e0d176cc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Starting QMC with 1 XLA devices per host across 1 hosts.\n",
      "INFO:absl:No checkpoint found. Training new model.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "unzip2 was deprecated in JAX v0.6.0 and removed in JAX v0.7.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m cfg.batch_size = \u001b[32m256\u001b[39m\n\u001b[32m     19\u001b[39m cfg.pretrain.iterations = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Parv/Doc/RA/Projects/QVMC/ferminet/ferminet/train.py:787\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(cfg, writer_manager)\u001b[39m\n\u001b[32m    784\u001b[39m   local_energy = local_energy_fn\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cfg.optim.objective == \u001b[33m'\u001b[39m\u001b[33mvmc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m   evaluate_loss = \u001b[43mqmc_loss_functions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m      \u001b[49m\u001b[43mlog_network\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_complex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogabs_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m      \u001b[49m\u001b[43mlocal_energy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m      \u001b[49m\u001b[43mclip_local_energy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_local_energy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m      \u001b[49m\u001b[43mclip_from_median\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_median\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcenter_at_clipped_energy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcenter_at_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomplex_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmax_vmap_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_vmap_batch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m cfg.optim.objective == \u001b[33m'\u001b[39m\u001b[33mwqmc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    797\u001b[39m   evaluate_loss = qmc_loss_functions.make_wqmc_loss(\n\u001b[32m    798\u001b[39m       log_network \u001b[38;5;28;01mif\u001b[39;00m use_complex \u001b[38;5;28;01melse\u001b[39;00m logabs_network,\n\u001b[32m    799\u001b[39m       local_energy,\n\u001b[32m   (...)\u001b[39m\u001b[32m    805\u001b[39m       vmc_weight=cfg.optim.get(\u001b[33m'\u001b[39m\u001b[33mvmc_weight\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m    806\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Parv/Doc/RA/Projects/QVMC/ferminet/ferminet/loss.py:189\u001b[39m, in \u001b[36mmake_loss\u001b[39m\u001b[34m(network, local_energy, clip_local_energy, clip_from_median, center_at_clipped_energy, complex_output, max_vmap_batch_size)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates the loss function, including custom gradients.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m \u001b[33;03m  loss is averaged over the batch and over all devices inside a pmap.\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    187\u001b[39m vmap = jax.vmap \u001b[38;5;28;01mif\u001b[39;00m max_vmap_batch_size == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m functools.partial(\n\u001b[32m    188\u001b[39m     folx.batched_vmap, max_batch_size=max_vmap_batch_size)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m batch_local_energy = \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_energy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnetworks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFermiNetData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspins\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matoms\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m batch_network = vmap(network, in_axes=(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m), out_axes=\u001b[32m0\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;129m@jax\u001b[39m.custom_jvp\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtotal_energy\u001b[39m(\n\u001b[32m    202\u001b[39m     params: networks.ParamTree,\n\u001b[32m    203\u001b[39m     key: chex.PRNGKey,\n\u001b[32m    204\u001b[39m     data: networks.FermiNetData,\n\u001b[32m    205\u001b[39m ) -> Tuple[jnp.ndarray, AuxiliaryLossData]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Parv/Doc/RA/Projects/QVMC/fermi/lib/python3.12/site-packages/jax/_src/api.py:1077\u001b[39m, in \u001b[36mvmap\u001b[39m\u001b[34m(fun, in_axes, out_axes, axis_name, axis_size, spmd_axis_name)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (in_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(in_axes) \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, *batching.spec_types}):\n\u001b[32m   1075\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mvmap in_axes must be an int, None, or a tuple of entries corresponding \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1076\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto the positional arguments passed to the function, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_axes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mtype\u001b[39m(l) \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28mint\u001b[39m, *batching.spec_types} \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtree_leaves\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m   1078\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mvmap in_axes must be an int, None, or (nested) container \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1079\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith those types as leaves, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_axes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mtype\u001b[39m(l) \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28mint\u001b[39m, *batching.spec_types} \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m tree_leaves(out_axes)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Parv/Doc/RA/Projects/QVMC/fermi/lib/python3.12/site-packages/jax/_src/tree_util.py:92\u001b[39m, in \u001b[36mtree_leaves\u001b[39m\u001b[34m(tree, is_leaf)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;129m@export\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtree_leaves\u001b[39m(tree: Any,\n\u001b[32m     89\u001b[39m                 is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     90\u001b[39m                 ) -> \u001b[38;5;28mlist\u001b[39m[Leaf]:\n\u001b[32m     91\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Alias of :func:`jax.tree.leaves`.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Parv/Doc/RA/Projects/QVMC/fermi/lib/python3.12/site-packages/chex/_src/dataclass.py:302\u001b[39m, in \u001b[36mregister_dataclass_type_with_jax_tree_util.<locals>.<lambda>\u001b[39m\u001b[34m(d)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;129m@functools\u001b[39m.cache\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mregister_dataclass_type_with_jax_tree_util\u001b[39m(data_class):\n\u001b[32m    290\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Register an existing dataclass so JAX knows how to handle it.\u001b[39;00m\n\u001b[32m    291\u001b[39m \n\u001b[32m    292\u001b[39m \u001b[33;03m  This means that functions in jax.tree_util operate over the fields\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m \u001b[33;03m      in instance.__dict__.\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m   flatten = \u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43munzip2\u001b[49m(\u001b[38;5;28msorted\u001b[39m(d.\u001b[34m__dict__\u001b[39m.items()))[::-\u001b[32m1\u001b[39m]\n\u001b[32m    303\u001b[39m   unflatten = functools.partial(_dataclass_unflatten, data_class)\n\u001b[32m    304\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Parv/Doc/RA/Projects/QVMC/fermi/lib/python3.12/site-packages/jax/_src/deprecations.py:54\u001b[39m, in \u001b[36mdeprecation_getattr.<locals>.getattr\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     52\u001b[39m message, fn = deprecations[name]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Is the deprecation accelerated?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(message)\n\u001b[32m     55\u001b[39m warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[31mAttributeError\u001b[39m: unzip2 was deprecated in JAX v0.6.0 and removed in JAX v0.7.0."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from absl import logging\n",
    "from ferminet.utils import system\n",
    "from ferminet import base_config\n",
    "from ferminet import train\n",
    "\n",
    "# Optional, for also printing training progress to STDOUT.\n",
    "# If running a script, you can also just use the --alsologtostderr flag.\n",
    "logging.get_absl_handler().python_handler.stream = sys.stdout\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "# Define H2 molecule\n",
    "cfg = base_config.default()\n",
    "cfg.system.electrons = (1,1)  # (alpha electrons, beta electrons)\n",
    "cfg.system.molecule = [system.Atom('H', (0, 0, -1)), system.Atom('H', (0, 0, 1))]\n",
    "\n",
    "# Set training parameters\n",
    "cfg.batch_size = 256\n",
    "cfg.pretrain.iterations = 0\n",
    "\n",
    "train.train(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c313d98-478c-4033-8902-670c82712eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-name)",
   "language": "python",
   "name": "venv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
