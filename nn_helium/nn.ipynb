{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9273957a",
   "metadata": {},
   "source": [
    "## Starter notebook for MLP ansatz for Helium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64533c",
   "metadata": {},
   "source": [
    "1. Energy - using Hessian,\n",
    "2. Gradients - using known formula (update manually),\n",
    "3. Optimization - ADAM.\n",
    "\n",
    "First, non-symmetric, without Jastrow factor, to slowly add complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d0d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import vmap\n",
    "from functorch import make_functional, vmap, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c2d90",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dad35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, n_hidden_layers, hidden_dim, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # Output layer (no activation here by default)\n",
    "        layers.append(nn.Linear(hidden_dim, output_size))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a25835cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 6\n",
    "n_hidden_layers = 2\n",
    "hidden_dim = 32\n",
    "output_size = 1\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=input_dim,\n",
    "    n_hidden_layers=n_hidden_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_size=output_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "848ea277",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce9b0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(x):\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x.to(device)\n",
    "\n",
    "    return model(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227514a",
   "metadata": {},
   "source": [
    "### Metropolis sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea11922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(N: int, n_runs: int):  \n",
    "\n",
    "    L = 1\n",
    "    r1 = (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "    r2 = (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "    sampled_Xs = []\n",
    "    rejection_ratio = 0\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        chose = torch.rand(n_runs).reshape(n_runs, 1)\n",
    "        dummy = torch.rand(n_runs)\n",
    "\n",
    "        perturbed_r1 = r1 + 0.5 * (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "        perturbed_r2 = r2 + 0.5 * (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "\n",
    "        r1_trial = torch.where(chose < 0.5, perturbed_r1, r1)\n",
    "        r2_trial = torch.where(chose >= 0.5, perturbed_r2, r2)\n",
    "        psi_val = psi(torch.cat((r1, r2), axis=1))\n",
    "        psi_trial_val = psi(torch.cat((r1_trial, r2_trial), axis=1))\n",
    "\n",
    "        \n",
    "        psi_ratio = (psi_trial_val / psi_val) ** 2\n",
    "\n",
    "        dummy_comp = psi_ratio > dummy\n",
    "\n",
    "        condition = dummy_comp\n",
    "\n",
    "        rejection_ratio += torch.where(condition, 1./N, 0.0)\n",
    "\n",
    "        condition = condition.reshape(condition.shape[0], 1)\n",
    "\n",
    "        r1 = torch.where(condition, r1_trial, r1)\n",
    "        r2 = torch.where(condition, r2_trial, r2)\n",
    "                \n",
    "    \n",
    "        sampled_Xs.append(torch.cat((r1, r2), axis=1))\n",
    "\n",
    "    return torch.stack(sampled_Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84de57",
   "metadata": {},
   "source": [
    "### Local energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "675510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_energy(positions):\n",
    "    \n",
    "    positions.requires_grad_(True)\n",
    "    psi_val = psi(positions)\n",
    "\n",
    "    grads = torch.autograd.grad(psi_val.sum(), positions, create_graph=True)[0]\n",
    "\n",
    "    laplacian = 0\n",
    "    for i in range(positions.shape[1]):\n",
    "        grad_i = grads[:, i]\n",
    "        grad2 = torch.autograd.grad(grad_i.sum(), positions, create_graph=True)[0][:, i]\n",
    "        laplacian += grad2\n",
    "\n",
    "    kinetic = -0.5 * (laplacian) / psi_val \n",
    "\n",
    "    r1 = positions[:, 0:3]\n",
    "    r2 = positions[:, 3:6]\n",
    "    r1_norm = torch.norm(r1, dim=1)\n",
    "    r2_norm = torch.norm(r2, dim=1)\n",
    "    r12 = torch.norm(r1 - r2, dim=1)\n",
    "\n",
    "    potential = -2 / r1_norm - 2 / r2_norm + 1 / r12\n",
    "\n",
    "    E_local = kinetic + potential\n",
    "    return E_local\n",
    "\n",
    "\n",
    "def get_local_energy(sampled_Xs):\n",
    "\n",
    "    mc_steps = sampled_Xs.shape[0]\n",
    "    walkers = sampled_Xs.shape[1]\n",
    "    reshaped_Xs = sampled_Xs.permute(1, 0, 2) # N_walkers, N, input_dim\n",
    "    flattened_Xs = reshaped_Xs.flatten(end_dim=1) # N_walkers * N, input_dim\n",
    "    local_E = local_energy(flattened_Xs) # N_walkers * N, 1\n",
    "    return local_E.reshape(walkers, mc_steps)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa37532",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8ed2efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parameter_gradients(x, E, network, n_walkers, mc_steps):\n",
    "    \n",
    "    fmodel, params = make_functional(network)\n",
    "\n",
    "    def psi_func(params, x):\n",
    "        return fmodel(params, x.unsqueeze(0)).squeeze()\n",
    "\n",
    "    grad_log_psi = grad(psi_func)\n",
    "\n",
    "    grads = vmap(grad_log_psi, in_dims=(None, 0))(params, x)\n",
    "    flat_grads = torch.cat([g.reshape(x.shape[0], -1) for g in grads], dim=1)\n",
    "\n",
    "    n_parameters = flat_grads.shape[-1]\n",
    "\n",
    "    mean_grad = flat_grads.mean(dim=1)\n",
    "    mean_E = E.mean(dim=1, keepdim=True)\n",
    "\n",
    "    centered_E = E - mean_E\n",
    "    centered_grads = flat_grads - mean_grad.unsqueeze(1)\n",
    "\n",
    "    return torch.mean(((centered_grads.T * centered_E.flatten())).reshape(n_walkers, mc_steps, n_parameters), axis=1)\n",
    "\n",
    "def get_parameter_gradients(sampled_Xs, local_E, network):\n",
    "\n",
    "    mc_steps = sampled_Xs.shape[0]\n",
    "    n_walkers = sampled_Xs.shape[1]\n",
    "    reshaped_Xs = sampled_Xs.permute(1, 0, 2) # N_walkers, N, input_dim\n",
    "    flattened_Xs = reshaped_Xs.flatten(end_dim=1) # N_walkers * N, input_dim\n",
    "\n",
    "    return parameter_gradients(flattened_Xs, local_E, network, n_walkers, mc_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e226ce8",
   "metadata": {},
   "source": [
    "### Assigning gradients to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "55f3ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_gradients_to_model(parameter_gradients, model):\n",
    "    \"\"\"Assign a flattened gradient vector to model parameters.\"\"\"\n",
    "    pointer = 0\n",
    "    for p in model.parameters():\n",
    "        numel = p.numel()\n",
    "        p.grad = parameter_gradients[pointer:pointer + numel].view_as(p).clone()\n",
    "        pointer += numel\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769655f",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "17dd55f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1965.27it/s]\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:100: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('make_functional', 'torch.func.functional_call')\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:65: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.grad is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.grad instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('grad')\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.28386828303337097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1870.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.23376259207725525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1906.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.23676703870296478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1887.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.28802844882011414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1944.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.27324503660202026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1773.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.28660693764686584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1955.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.21810366213321686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1944.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.22769327461719513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1936.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.2886466681957245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1947.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.27718785405158997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1797.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean energy is -0.23017017543315887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[212], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m local_E \u001b[38;5;241m=\u001b[39m get_local_energy(sampled_Xs)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean energy is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mmean(local_E)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mget_parameter_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_Xs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_E\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m assign_gradients_to_model(grads[\u001b[38;5;241m0\u001b[39m], model)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[182], line 30\u001b[0m, in \u001b[0;36mget_parameter_gradients\u001b[1;34m(sampled_Xs, local_E, network)\u001b[0m\n\u001b[0;32m     27\u001b[0m reshaped_Xs \u001b[38;5;241m=\u001b[39m sampled_Xs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# N_walkers, N, input_dim\u001b[39;00m\n\u001b[0;32m     28\u001b[0m flattened_Xs \u001b[38;5;241m=\u001b[39m reshaped_Xs\u001b[38;5;241m.\u001b[39mflatten(end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# N_walkers * N, input_dim\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparameter_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_Xs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_E\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_walkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[182], line 11\u001b[0m, in \u001b[0;36mparameter_gradients\u001b[1;34m(x, E, network, n_walkers, mc_steps)\u001b[0m\n\u001b[0;32m      8\u001b[0m grad_log_psi \u001b[38;5;241m=\u001b[39m grad(psi_func)\n\u001b[0;32m     10\u001b[0m grads \u001b[38;5;241m=\u001b[39m vmap(grad_log_psi, in_dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m))(params, x)\n\u001b[1;32m---> 11\u001b[0m flat_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m n_parameters \u001b[38;5;241m=\u001b[39m flat_grads\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     15\u001b[0m mean_grad \u001b[38;5;241m=\u001b[39m flat_grads\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    N = 5000\n",
    "    n_walkers = 5\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    sampled_Xs = metropolis(N, n_walkers)[500:]\n",
    "\n",
    "    local_E = get_local_energy(sampled_Xs)\n",
    "    print(f\"Mean energy is {torch.mean(local_E)}\")\n",
    "\n",
    "    grads = get_parameter_gradients(sampled_Xs, local_E, model.to(cpu))\n",
    "    model = assign_gradients_to_model(grads[0], model)\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06718e62",
   "metadata": {},
   "source": [
    "Incoporate skipping the first n steps in local energy and gradient calulations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
