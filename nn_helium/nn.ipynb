{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9273957a",
   "metadata": {},
   "source": [
    "## Starter notebook for MLP ansatz for Helium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64533c",
   "metadata": {},
   "source": [
    "1. Energy - using Hessian,\n",
    "2. Gradients - using known formula (update manually),\n",
    "3. Optimization - ADAM.\n",
    "\n",
    "First, non-symmetric, without Jastrow factor, to slowly add complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "76d0d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4dad35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, n_hidden_layers, hidden_dim, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # Output layer (no activation here by default)\n",
    "        layers.append(nn.Linear(hidden_dim, output_size))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a25835cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 6\n",
    "n_hidden_layers = 2\n",
    "hidden_dim = 32\n",
    "output_size = 1\n",
    "\n",
    "network = MLP(\n",
    "    input_dim=input_dim,\n",
    "    n_hidden_layers=n_hidden_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_size=output_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea11922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(N: int, n_runs: int, model: nn.Module):  \n",
    "    \"\"\"\n",
    "    Vectorized metropolis loop\n",
    "    Over N steps, for n_runs. \n",
    "    Alphas passes in must be of same dim as n_runs\n",
    "    \"\"\"       \n",
    "    L = 1\n",
    "    r1 = (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "    r2 = (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "    max_steps = 500\n",
    "    sampled_Xs = []\n",
    "    rejection_ratio = 0\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        chose = torch.rand(n_runs).reshape(n_runs, 1)\n",
    "        dummy = torch.rand(n_runs)\n",
    "\n",
    "        perturbed_r1 = r1 + 0.5 * (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "        perturbed_r2 = r2 + 0.5 * (torch.rand(n_runs, 3) * 2 * L - L)\n",
    "\n",
    "        r1_trial = torch.where(chose < 0.5, perturbed_r1, r1)\n",
    "        r2_trial = torch.where(chose >= 0.5, perturbed_r2, r2)\n",
    "        psi_val = model(torch.cat((r1, r2), axis=1)).squeeze()\n",
    "        psi_trial_val = model(torch.cat((r1_trial, r2_trial), axis=1)).squeeze()\n",
    "\n",
    "        \n",
    "        psi_ratio = (psi_trial_val / psi_val) ** 2\n",
    "\n",
    "        dummy_comp = psi_ratio > dummy\n",
    "\n",
    "        condition = dummy_comp\n",
    "\n",
    "        rejection_ratio += torch.where(condition, 1./N, 0.0)\n",
    "\n",
    "        condition = condition.reshape(condition.shape[0], 1)\n",
    "\n",
    "        r1 = torch.where(condition, r1_trial, r1)\n",
    "        r2 = torch.where(condition, r2_trial, r2)\n",
    "                \n",
    "        if i > max_steps:\n",
    "            sampled_Xs.append(torch.cat((r1, r2), axis=1))\n",
    "\n",
    "    return torch.stack(sampled_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "675510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the simplest one - all the positions\n",
    "\n",
    "def local_energy(positions):\n",
    "    # positions: [batch_size, 6] with [r1x, r1y, r1z, r2x, r2y, r2z]\n",
    "    psi = network(positions).squeeze()\n",
    "\n",
    "    # Gradient of log_psi w.r.t positions\n",
    "    grads = torch.autograd.grad(psi.sum(), positions, create_graph=True)[0]\n",
    "\n",
    "    # Laplacian: second derivative (sum of second partials)\n",
    "    laplacian = 0\n",
    "    for i in range(positions.shape[1]):\n",
    "        grad_i = grads[:, i]\n",
    "        grad2 = torch.autograd.grad(grad_i.sum(), positions, create_graph=True)[0][:, i]\n",
    "        laplacian += grad2\n",
    "\n",
    "    # Kinetic energy\n",
    "    kinetic = -0.5 * (laplacian) / psi \n",
    "\n",
    "    # Reshape positions\n",
    "    r1 = positions[:, 0:3]\n",
    "    r2 = positions[:, 3:6]\n",
    "    r1_norm = torch.norm(r1, dim=1)\n",
    "    r2_norm = torch.norm(r2, dim=1)\n",
    "    r12 = torch.norm(r1 - r2, dim=1)\n",
    "\n",
    "    potential = -2 / r1_norm - 2 / r2_norm + 1 / r12\n",
    "\n",
    "    E_local = kinetic + potential\n",
    "    return E_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cecf2a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1961.71it/s]\n"
     ]
    }
   ],
   "source": [
    "sampled_Xs = metropolis(5000, 50, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a4cb626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_es = torch.stack([local_energy(sampled_Xs[:, i]) for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b165ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_gradients(model, x):\n",
    "    # x: [batch_size, 6] electron positions\n",
    "    psi = model(x).squeeze()\n",
    "    \n",
    "    # Compute gradients of log_psi w.r.t. model parameters\n",
    "    grads = torch.autograd.grad(\n",
    "        psi.sum(),                         # scalar output\n",
    "        model.parameters(),                # list of α (weights & biases)\n",
    "        retain_graph=True,                 # keep graph for backprop later\n",
    "        create_graph=True                  # allows higher-order grads (if needed)\n",
    "    )\n",
    "    \n",
    "    # grads is a tuple of parameter gradients; flatten into one vector per sample\n",
    "    grad_psi = torch.cat([g.view(-1) for g in grads])\n",
    "    \n",
    "    return grad_psi\n",
    "\n",
    "def get_gradients(model, x):\n",
    "\n",
    "    grad_psi = psi_gradients(model, x)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1af6c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.stack([log_psi_gradients(network, sampled_Xs[:, i]) for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "700cc143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1313])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b417df",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial \\theta_\\alpha} = \n",
    "2\\, \\text{Re} \\left\\langle \n",
    "\\frac{\\partial \\ln \\Psi}{\\partial \\theta_\\alpha} \n",
    "\\left( E_\\text{loc} - E[\\boldsymbol{\\theta}] \\right) \n",
    "\\right\\rangle\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30989f77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
